# -*- coding: utf-8 -*-
"""TASK.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1O3Cywm_O76gZ-cGQGmx5RKs2FTO96il0
"""

!pip install sentence-transformers sklearn numpy

# Commented out IPython magic to ensure Python compatibility.
# %%writefile narrative_builder.py
# #!/usr/bin/env python
# import argparse
# import json
# import math
# from collections import defaultdict
# from datetime import datetime
# from typing import List, Dict, Any, Tuple
# 
# import numpy as np
# from sentence_transformers import SentenceTransformer
# from sklearn.cluster import KMeans
# from sklearn.metrics.pairwise import cosine_similarity
# 
# 
# def load_news(path: str) -> List[Dict[str, Any]]:
#     """
#     More flexible loader:
#     - list of articles
#     - {"articles": [...]}
#     - {"data": [...]}
#     - biggest list field in a dict
#     - JSON Lines (one JSON object per line)
#     """
#     # Try normal JSON first
#     try:
#         with open(path, "r", encoding="utf-8") as f:
#             data = json.load(f)
#     except json.JSONDecodeError:
#         # Try JSON Lines (one object per line)
#         articles = []
#         with open(path, "r", encoding="utf-8") as f:
#             for line in f:
#                 line = line.strip()
#                 if not line:
#                     continue
#                 try:
#                     obj = json.loads(line)
#                     if isinstance(obj, dict):
#                         articles.append(obj)
#                 except json.JSONDecodeError:
#                     continue
#         if not articles:
#             raise ValueError("Could not parse news file as JSON or JSON Lines.")
#         print(f"[load_news] Loaded {len(articles)} articles from JSON Lines.")
#         return articles
# 
#     # Case 1: already a list of objects
#     if isinstance(data, list):
#         print(f"[load_news] Loaded {len(data)} articles from top-level list.")
#         return data
# 
#     # Case 2: dict with 'articles'
#     if isinstance(data, dict):
#         if "articles" in data and isinstance(data["articles"], list):
#             print(f"[load_news] Loaded {len(data['articles'])} articles from 'articles' key.")
#             return data["articles"]
# 
#         # Common alternative key names
#         for key in ["data", "items", "results", "docs"]:
#             if key in data and isinstance(data[key], list):
#                 print(f"[load_news] Loaded {len(data[key])} articles from '{key}' key.")
#                 return data[key]
# 
#         # Fallback: choose the largest list value in the dict
#         list_candidates = [
#             v for v in data.values()
#             if isinstance(v, list) and v and isinstance(v[0], dict)
#         ]
#         if list_candidates:
#             biggest = max(list_candidates, key=len)
#             print(f"[load_news] Loaded {len(biggest)} articles from largest list field in dict.")
#             return biggest
# 
#     raise ValueError("Unexpected JSON structure. Tried several formats and could not find article list.")
# 
# 
# def filter_by_source_rating(articles: List[Dict[str, Any]], min_rating: float = 8.0) -> List[Dict[str, Any]]:
#     result = []
#     for a in articles:
#         rating = a.get("source_rating", 0.0)
#         try:
#             rating = float(rating)
#         except (TypeError, ValueError):
#             rating = 0.0
#         if rating > min_rating:
#             result.append(a)
#     print(f"[filter_by_source_rating] {len(result)} / {len(articles)} articles kept (rating > {min_rating}).")
#     return result if result else articles  # fallback: if nothing left, use all
# 
# 
# class NewsEmbedder:
#     def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
#         self.model = SentenceTransformer(model_name)
# 
#     def article_text(self, article: Dict[str, Any]) -> str:
#         parts = [
#             article.get("title", ""),
#             article.get("description", ""),
#             article.get("content", ""),
#         ]
#         return " ".join([p for p in parts if p]).strip()
# 
#     def embed_articles(self, articles: List[Dict[str, Any]]) -> np.ndarray:
#         texts = [self.article_text(a) for a in articles]
#         embeddings = self.model.encode(texts, show_progress_bar=True)
#         return np.array(embeddings)
# 
#     def embed_query(self, query: str) -> np.ndarray:
#         return np.array(self.model.encode([query]))
# 
# 
# def select_relevant_articles(
#     articles: List[Dict[str, Any]],
#     embeddings: np.ndarray,
#     topic: str,
#     embedder: NewsEmbedder,
#     top_k: int = 100,
#     similarity_threshold: float = 0.35,
# ) -> Tuple[List[Dict[str, Any]], np.ndarray]:
#     q_emb = embedder.embed_query(topic)
#     sims = cosine_similarity(q_emb, embeddings)[0]
# 
#     ranked_indices = np.argsort(-sims)
# 
#     selected_indices = []
#     for idx in ranked_indices:
#         if sims[idx] < similarity_threshold:
#             break
#         selected_indices.append(idx)
#         if len(selected_indices) >= top_k:
#             break
# 
#     if not selected_indices:
#         selected_indices = ranked_indices[: min(top_k, len(ranked_indices))]
# 
#     selected_articles = [articles[i] for i in selected_indices]
#     selected_embs = embeddings[selected_indices]
#     print(f"[select_relevant_articles] Selected {len(selected_articles)} relevant articles.")
#     return selected_articles, selected_embs
# 
# 
# def parse_date(date_str: str) -> datetime:
#     if not date_str:
#         return datetime(1970, 1, 1)
# 
#     formats = [
#         "%Y-%m-%d",
#         "%Y-%m-%dT%H:%M:%S",
#         "%Y-%m-%dT%H:%M:%SZ",
#         "%Y-%m-%d %H:%M:%S",
#     ]
#     for fmt in formats:
#         try:
#             return datetime.strptime(date_str[:19], fmt)
#         except ValueError:
#             continue
#     return datetime(1970, 1, 1)
# 
# 
# def article_headline(a: Dict[str, Any]) -> str:
#     return a.get("title") or a.get("headline") or "(no title)"
# 
# 
# def article_text_short(a: Dict[str, Any], max_chars: int = 220) -> str:
#     text = a.get("content") or a.get("description") or a.get("title") or ""
#     text = text.replace("\n", " ")
#     if len(text) > max_chars:
#         text = text[: max_chars - 3] + "..."
#     return text
# 
# 
# def build_narrative_summary(
#     topic: str,
#     articles: List[Dict[str, Any]],
#     q_emb: np.ndarray,
#     article_embs: np.ndarray,
#     max_sentences: int = 8,
# ) -> str:
#     if not articles:
#         return f"No strong matches found for topic '{topic}'. The dataset may not contain relevant coverage."
# 
#     sims = cosine_similarity(q_emb, article_embs)[0]
#     order = np.argsort(-sims)
# 
#     n = min(max_sentences, len(articles))
#     chosen_indices = order[:n]
# 
#     sentences = []
#     for idx in chosen_indices:
#         a = articles[idx]
#         date = parse_date(a.get("date", "")).strftime("%Y-%m-%d")
#         headline = article_headline(a)
#         source = a.get("source_name", "Unknown source")
#         s = f"On {date}, {source} reported: \"{headline}\""
#         sentences.append(s)
# 
#     intro = f"This narrative summarizes how the topic '{topic}' unfolds across the filtered news sources."
#     summary = " ".join([intro] + sentences)
#     return summary
# 
# 
# def build_timeline(articles: List[Dict[str, Any]]):
#     sorted_articles = sorted(articles, key=lambda a: parse_date(a.get("date", "")))
# 
#     timeline = []
#     for a in sorted_articles:
#         date_obj = parse_date(a.get("date", ""))
#         date_str = date_obj.strftime("%Y-%m-%d")
#         headline = article_headline(a)
#         url = a.get("url", "")
#         why = f"This article from {a.get('source_name', 'Unknown source')} adds an important piece to the evolving story: {article_text_short(a)}"
#         timeline.append(
#             {
#                 "date": date_str,
#                 "headline": headline,
#                 "url": url,
#                 "why_it_matters": why,
#             }
#         )
#     return timeline
# 
# 
# def choose_num_clusters(n_articles: int) -> int:
#     if n_articles <= 3:
#         return 1
#     k = int(round(math.sqrt(n_articles)))
#     return max(2, min(8, k))
# 
# 
# def build_clusters(articles: List[Dict[str, Any]], embeddings: np.ndarray):
#     n = len(articles)
#     if n == 0:
#         return []
# 
#     if n <= 2:
#         return [
#             {
#                 "cluster_id": 0,
#                 "label": "All relevant articles",
#                 "article_ids": [i for i in range(n)],
#             }
#         ]
# 
#     k = choose_num_clusters(n)
#     kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
#     labels = kmeans.fit_predict(embeddings)
# 
#     clusters_raw: Dict[int, List[int]] = defaultdict(list)
#     for idx, label in enumerate(labels):
#         clusters_raw[int(label)].append(idx)
# 
#     clusters_out = []
#     for cid, idxs in clusters_raw.items():
#         representative_idx = idxs[0]
#         rep_headline = article_headline(articles[representative_idx])
#         label = f"Theme {cid+1}: {rep_headline[:60]}"
# 
#         clusters_out.append(
#             {
#                 "cluster_id": int(cid),
#                 "label": label,
#                 "article_ids": [int(i) for i in idxs],
#             }
#         )
# 
#     return clusters_out
# 
# 
# def detect_relation(a1: Dict[str, Any], a2: Dict[str, Any], sim: float) -> str:
#     date1 = parse_date(a1.get("date", ""))
#     date2 = parse_date(a2.get("date", ""))
#     newer = date2 > date1
# 
#     text2 = (a2.get("content") or "").lower()
# 
#     contradiction_keywords = ["denies", "refutes", "contradicts", "disputes", "clarifies"]
#     if any(k in text2 for k in contradiction_keywords) and sim > 0.4:
#         return "contradicts"
# 
#     if sim > 0.65 and newer:
#         escalate_keywords = ["clashes", "attack", "escalates", "tensions", "violence", "sanctions"]
#         if any(k in text2 for k in escalate_keywords):
#             return "escalates"
#         return "builds_on"
# 
#     return "adds_context"
# 
# 
# def build_graph(articles: List[Dict[str, Any]], embeddings: np.ndarray, max_edges_per_node: int = 4):
#     n = len(articles)
#     if n == 0:
#         return {"nodes": [], "edges": []}
# 
#     sims = cosine_similarity(embeddings)
# 
#     nodes = []
#     edges = []
# 
#     for i, a in enumerate(articles):
#         nodes.append(
#             {
#                 "id": i,
#                 "headline": article_headline(a),
#                 "date": parse_date(a.get("date", "")).strftime("%Y-%m-%d"),
#                 "url": a.get("url", ""),
#             }
#         )
# 
#     for i in range(n):
#         sim_row = sims[i].copy()
#         sim_row[i] = -1.0
#         idx_sorted = np.argsort(-sim_row)
# 
#         added = 0
#         for j in idx_sorted:
#             if sim_row[j] <= 0:
#                 break
#             relation = detect_relation(articles[i], articles[j], sim_row[j])
#             edges.append(
#                 {
#                     "source": int(i),
#                     "target": int(j),
#                     "relation": relation,
#                     "similarity": float(sim_row[j]),
#                 }
#             )
#             added += 1
#             if added >= max_edges_per_node:
#                 break
# 
#     return {"nodes": nodes, "edges": edges}
# 
# 
# def build_narrative(topic: str, data_path: str) -> Dict[str, Any]:
#     articles = load_news(data_path)
#     print(f"[build_narrative] Total articles loaded: {len(articles)}")
# 
#     filtered = filter_by_source_rating(articles, min_rating=8.0)
# 
#     embedder = NewsEmbedder()
#     all_embs = embedder.embed_articles(filtered)
# 
#     relevant_articles, relevant_embs = select_relevant_articles(
#         filtered, all_embs, topic, embedder,
#         top_k=150,
#         similarity_threshold=0.35
#     )
# 
#     if not relevant_articles:
#         return {
#             "narrative_summary": f"No relevant articles found for topic '{topic}'.",
#             "timeline": [],
#             "clusters": [],
#             "graph": {"nodes": [], "edges": []},
#         }
# 
#     q_emb = embedder.embed_query(topic)
#     narrative_summary = build_narrative_summary(
#         topic, relevant_articles, q_emb, relevant_embs
#     )
#     timeline = build_timeline(relevant_articles)
#     clusters = build_clusters(relevant_articles, relevant_embs)
#     graph = build_graph(relevant_articles, relevant_embs)
# 
#     result = {
#         "narrative_summary": narrative_summary,
#         "timeline": timeline,
#         "clusters": clusters,
#         "graph": graph,
#     }
#     return result
# 
# 
# def parse_args():
#     parser = argparse.ArgumentParser(
#         description="Narrative Builder from News Dataset"
#     )
#     parser.add_argument(
#         "--topic",
#         type=str,
#         required=True,
#         help="Topic string, e.g. 'Jubilee Hills elections'",
#     )
#     parser.add_argument(
#         "--data_path",
#         type=str,
#         default="news.json",
#         help="Path to JSON news dataset.",
#     )
#     return parser.parse_args()
# 
# 
# def main():
#     args = parse_args()
#     result = build_narrative(topic=args.topic, data_path=args.data_path)
#     print(json.dumps(result, indent=2))
# 
# 
# if __name__ == "__main__":
#     main()

!python narrative_builder.py --topic "Jubilee Hills elections"

!python narrative_builder.py --topic "Israel-Iran conflict"

! python narrative_builder.py --topic "AI regulation"